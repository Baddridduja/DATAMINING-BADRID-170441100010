{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"k-mean/","text":"K-mean Clustring Pengertian K-mean Clustring K-means Clustering adalah salah satu \u201cunsupervised machine learning algorithms\u201d yang paling sederhana dan populer .Tujuan dari algoritma ini adalah untuk menemukan grup dalam data, dengan jumlah grup yang diwakili oleh variabel K. Variabel K sendiri adalah jumlah cluster yang kita inginkan,Metode k-mean clustering ini mempartisi data ke dalam cluster sehingga data yang memiliki karakteristik yang sama dikelompokkan ke dalam satu cluster yang sama dan data yang mempunyai karateristik yang berbeda di kelompokan ke dalam cluster yang lain. Proses membuat k-mean clustring Terdapat dua jenis data clustering yang sering dipergunakan dalam proses pengelompokan data yaitu Hierarchical dan Non-Hierarchical , dan K-Means merupakan salah satu metode data clustering non-hierarchical atau Partitional Clustering . Metode K-Means Clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Dengan kata lain, metode K-Means Clustering bertujuan untuk meminimalisasikan objective function yang diset dalam proses clustering dengan cara meminimalkan variasi antar data yang ada di dalam suatu cluster dan memaksimalkan variasi dengan data yang ada di cluster lainnya . Data clustering menggunakan metode K-Means Clustering ini secara umum dilakukan dengan algoritma dasar sebagai berikut: Tentukan jumlah cluster Alokasikan data ke dalam cluster secara random Hitung centroid/rata-rata dari data yang ada di masing-masing cluster Alokasikan masing-masing data ke centroid/rata-rata terdekat Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan Beberapa Permasalahan yang Terkait Dengan K-Means Clustering Beberapa permasalahan yang sering muncul pada saat menggunakan metode K-Means untuk melakukan pengelompokan data adalah: Ditemukannya beberapa model clustering yang berbeda Pemilihan jumlah cluster yang paling tepat Kegagalan untuk converge Outliers Bentuk cluster Overlapping # Project information site_name: 'Material for MkDocs' site_description: 'A Material Design theme for MkDocs' site_author: 'Martin Donath' site_url: 'https://squidfunk.github.io/mkdocs-material/' # Repository repo_name: 'squidfunk/mkdocs-material' repo_url: 'https://github.com/squidfunk/mkdocs-material' # Copyright copyright: 'Copyright copy; 2016 - 2017 Martin Donath' # Configuration theme: name: 'material' language: 'en' palette: primary: 'indigo' accent: 'indigo' font: text: 'Roboto' code: 'Roboto Mono' # Customization extra: manifest: 'manifest.webmanifest' social: - type: 'github' link: 'https://github.com/squidfunk' - type: 'twitter' link: 'https://twitter.com/squidfunk' - type: 'linkedin' link: 'https://linkedin.com/in/squidfunk' # Google Analytics google_analytics: - 'UA-XXXXXXXX-X' - 'auto' # Extensions markdown_extensions: - admonition - codehilite: guess_lang: false - toc: permalink: true STUDI KASUS Data pengunjung perpustakaan Langkah 1 : menyiapkan tools dan bahan mempersiapkan sebuah data untuk dijadikan clustring disini saya menggunakan data pengunjung perpustakaan yang berekstensi csv. tools yang digunakan : Anaconda spyder Langkah 2 : import libarary yang di butuhkan import numpy as np import pandas as pd import matplotlib.pyplot as plt Memanggil file CSV dataset=pd.read_csv('perpustakaan.csv') Menentukan fitur yang akan digunakan pada clustring X=dataset.iloc[:,[2,3]].values Langkah 3 : Membuat visualisasi clustring from sklearn.cluster import KMeans wcss=[] for i in range(1,11): kmeans=KMeans(n_clusters=i, init='k-means++',random_state=0) kmeans.fit(X) wcss.append(kmeans.inertia_) plt.plot(range(1,11),wcss) plt.title('the ebow method') plt.xlabel('number of cluster') plt.ylabel('-') plt.show() kmeans=KMeans(n_clusters=3,init='k-means++',random_state=0) y_kmeans=kmeans.fit_predict(X) plt.scatter(X[y_kmeans==0,0],X[y_kmeans==0,1],s=100,c='red',label='Cluster1') plt.scatter(X[y_kmeans==1,0],X[y_kmeans==1,1],s=100,c='blue',label='CLuster2') plt.scatter(X[y_kmeans==2,0],X[y_kmeans==2,1],s=100,c='green',label='Cluster3') plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=300,c='yellow',label='centroids') plt.title('Clusters of visior') plt.xlabel('loan amount') plt.ylabel('book code') plt.legend() plt.show() Output program :","title":"K-mean clutring"},{"location":"k-mean/#k-mean-clustring","text":"Pengertian K-mean Clustring K-means Clustering adalah salah satu \u201cunsupervised machine learning algorithms\u201d yang paling sederhana dan populer .Tujuan dari algoritma ini adalah untuk menemukan grup dalam data, dengan jumlah grup yang diwakili oleh variabel K. Variabel K sendiri adalah jumlah cluster yang kita inginkan,Metode k-mean clustering ini mempartisi data ke dalam cluster sehingga data yang memiliki karakteristik yang sama dikelompokkan ke dalam satu cluster yang sama dan data yang mempunyai karateristik yang berbeda di kelompokan ke dalam cluster yang lain.","title":"K-mean Clustring"},{"location":"k-mean/#proses-membuat-k-mean-clustring","text":"Terdapat dua jenis data clustering yang sering dipergunakan dalam proses pengelompokan data yaitu Hierarchical dan Non-Hierarchical , dan K-Means merupakan salah satu metode data clustering non-hierarchical atau Partitional Clustering . Metode K-Means Clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Dengan kata lain, metode K-Means Clustering bertujuan untuk meminimalisasikan objective function yang diset dalam proses clustering dengan cara meminimalkan variasi antar data yang ada di dalam suatu cluster dan memaksimalkan variasi dengan data yang ada di cluster lainnya . Data clustering menggunakan metode K-Means Clustering ini secara umum dilakukan dengan algoritma dasar sebagai berikut: Tentukan jumlah cluster Alokasikan data ke dalam cluster secara random Hitung centroid/rata-rata dari data yang ada di masing-masing cluster Alokasikan masing-masing data ke centroid/rata-rata terdekat Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan","title":"Proses membuat k-mean clustring"},{"location":"k-mean/#beberapa-permasalahan-yang-terkait-dengan-k-means-clustering","text":"Beberapa permasalahan yang sering muncul pada saat menggunakan metode K-Means untuk melakukan pengelompokan data adalah: Ditemukannya beberapa model clustering yang berbeda Pemilihan jumlah cluster yang paling tepat Kegagalan untuk converge Outliers Bentuk cluster Overlapping # Project information site_name: 'Material for MkDocs' site_description: 'A Material Design theme for MkDocs' site_author: 'Martin Donath' site_url: 'https://squidfunk.github.io/mkdocs-material/' # Repository repo_name: 'squidfunk/mkdocs-material' repo_url: 'https://github.com/squidfunk/mkdocs-material' # Copyright copyright: 'Copyright copy; 2016 - 2017 Martin Donath' # Configuration theme: name: 'material' language: 'en' palette: primary: 'indigo' accent: 'indigo' font: text: 'Roboto' code: 'Roboto Mono' # Customization extra: manifest: 'manifest.webmanifest' social: - type: 'github' link: 'https://github.com/squidfunk' - type: 'twitter' link: 'https://twitter.com/squidfunk' - type: 'linkedin' link: 'https://linkedin.com/in/squidfunk' # Google Analytics google_analytics: - 'UA-XXXXXXXX-X' - 'auto' # Extensions markdown_extensions: - admonition - codehilite: guess_lang: false - toc: permalink: true","title":"Beberapa Permasalahan yang Terkait Dengan K-Means Clustering"},{"location":"k-mean/#studi-kasus-data-pengunjung-perpustakaan","text":"","title":"STUDI KASUS  Data pengunjung perpustakaan"},{"location":"k-mean/#langkah-1-menyiapkan-tools-dan-bahan","text":"mempersiapkan sebuah data untuk dijadikan clustring disini saya menggunakan data pengunjung perpustakaan yang berekstensi csv. tools yang digunakan : Anaconda spyder","title":"Langkah 1 : menyiapkan tools dan bahan"},{"location":"k-mean/#langkah-2-import-libarary-yang-di-butuhkan","text":"import numpy as np import pandas as pd import matplotlib.pyplot as plt Memanggil file CSV dataset=pd.read_csv('perpustakaan.csv') Menentukan fitur yang akan digunakan pada clustring X=dataset.iloc[:,[2,3]].values","title":"Langkah 2 : import libarary yang di butuhkan"},{"location":"k-mean/#langkah-3-membuat-visualisasi-clustring","text":"from sklearn.cluster import KMeans wcss=[] for i in range(1,11): kmeans=KMeans(n_clusters=i, init='k-means++',random_state=0) kmeans.fit(X) wcss.append(kmeans.inertia_) plt.plot(range(1,11),wcss) plt.title('the ebow method') plt.xlabel('number of cluster') plt.ylabel('-') plt.show() kmeans=KMeans(n_clusters=3,init='k-means++',random_state=0) y_kmeans=kmeans.fit_predict(X) plt.scatter(X[y_kmeans==0,0],X[y_kmeans==0,1],s=100,c='red',label='Cluster1') plt.scatter(X[y_kmeans==1,0],X[y_kmeans==1,1],s=100,c='blue',label='CLuster2') plt.scatter(X[y_kmeans==2,0],X[y_kmeans==2,1],s=100,c='green',label='Cluster3') plt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=300,c='yellow',label='centroids') plt.title('Clusters of visior') plt.xlabel('loan amount') plt.ylabel('book code') plt.legend() plt.show()","title":"Langkah 3 : Membuat visualisasi clustring"},{"location":"k-mean/#output-program","text":"","title":"Output program :"},{"location":"knn/","text":"K-Nearest Neighbor K-nearest neighbors adalah algoritma yang berfungsi untuk melakukan klasifikasi suatu data berdasarkan data pembelajaran ( train data sets ), yang diambil dari k tetangga terdekatnya ( nearest neighbors ). Dengan k merupakan banyaknya tetangga terdekat. Cara Kerja Algoritma K-Nearest Neighbors (KNN) K-nearest neighbors melakukan klasifikasi dengan proyeksi data pembelajaran pada ruang berdimensi banyak. Ruang ini dibagi menjadi bagian-bagian yang merepresentasikan kriteria data pembelajaran. Setiap data pembelajaran direpresentasikan menjadi titik-titik c pada ruang dimensi banyak. KLASIFIKASI TERDEKAT (NEAREST NEIGHBOR CLASSIFICATION)** Data baru yang diklasifikasi selanjutnya diproyeksikan pada ruang dimensi banyak yang telah memuat titik-titik c data pembelajaran. Proses klasifikasi dilakukan dengan mencari titik c terdekat dari c-baru ( nearest neighbor ) . Teknik pencarian tetangga terdekat yang umum dilakukan dengan menggunakan formula jarak euclidean . Berikut beberapa formula yang digunakan dalam algoritma knn. Euclidean Distance Jarak Euclidean adalah formula untuk mencari jarak antara 2 titik dalam ruang dua dimensi. Hamming Distance Jarak Hamming adalah cara mencari jarak antar 2 titik yang dihitung dengan panjang vektor biner yang dibentuk oleh dua titik tersebut dalam block kode biner. Manhattan Distance Manhattan Distance atau Taxicab Geometri adalah formula untuk mencari jarak d antar 2 vektor p,q pada ruang dimensi n . Minkowski Distance Minkowski distance adalah formula pengukuran antar 2 titik pada ruang vektor normal yang merupakan hibridisasi yang mengeneralisasi euclidean distance dan mahattan distance. Teknik pencarian tetangga terdekat disesuaikan dengan dimensi data, proyeksi, dan kemudahan implementasi oleh pengguna. Banyaknya k Tetangga Terdekat Untuk menggunakan algoritma k nearest neighbors, perlu ditentukan banyaknya k tetangga terdekat yang digunakan untuk melakukan klasifikasi data baru. Banyaknya k, sebaiknya merupakan angka ganjil, misalnya k = 1, 2, 3, dan seterusnya. Penentuan nilai k dipertimbangkan berdasarkan banyaknya data yang ada dan ukuran dimensi yang dibentuk oleh data. Semakin banyak data yang ada, angka k yang dipilih sebaiknya semakin rendah. Namun, semakin besar ukuran dimensi data, angka k yang dipilih sebaiknya semakin tinggi. Algoritma K-Nearest Neighbors Tentukan k bilangan bulat positif berdasarkan ketersediaan data pembelajaran. Pilih tetangga terdekat dari data baru sebanyak k. Tentukan klasifikasi paling umum pada langkah (ii), dengan menggunakan frekuensi terbanyak. Keluaran klasifikasi dari data sampel baru. STUDI KASUS Data pengunjung perpustakaan menggunakan knn Langkah 1 : menyiapkan tools dan bahan mempersiapkan sebuah data untuk dijadikan knn disini saya menggunakan data pengunjung perpustakaan yang berekstensi csv. tools yang digunakan : python 2.7 Langkah 2 : import libarary yang di butuhkan import pandas as pd from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split Memanggil file CSV data_orj = pd.read_csv( perpustakaan.csv ) Mengabil data untuk percobaan data = data_orj.loc [:,'Visitor':'book code'] Input jumlah k terdekat inK = int(input( Masukkan K : )) Mencari knn dengan menggunakan sklearn neighbors clssifier Langkah 3 : Mencari knn dengan menggunakan sklearn neighbors clssifier knn = KNeighborsClassifier(n_neighbors = inK) x,y = data_knn.loc[:,data_knn.columns != 'book code'], data_knn.loc[:,'book code'] x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state = 42) knn.fit(x_train,y_train) prediction = knn.predict(x_test) print('KNN dengan (K =',inK,') Akurasinya adalah: ', knn.score(x_test,y_test)) print( ) datatest = pd.DataFrame(x_test) datatest[ book code ] = y_test datatest[ prediksi ] = prediction print ( Data Asli dan Prediksinya ) print (datatest) Output program :","title":"K-Nearest Neighbor"},{"location":"knn/#k-nearest-neighbor","text":"K-nearest neighbors adalah algoritma yang berfungsi untuk melakukan klasifikasi suatu data berdasarkan data pembelajaran ( train data sets ), yang diambil dari k tetangga terdekatnya ( nearest neighbors ). Dengan k merupakan banyaknya tetangga terdekat.","title":"K-Nearest Neighbor"},{"location":"knn/#cara-kerja-algoritma-k-nearest-neighbors-knn","text":"K-nearest neighbors melakukan klasifikasi dengan proyeksi data pembelajaran pada ruang berdimensi banyak. Ruang ini dibagi menjadi bagian-bagian yang merepresentasikan kriteria data pembelajaran. Setiap data pembelajaran direpresentasikan menjadi titik-titik c pada ruang dimensi banyak.","title":"Cara Kerja Algoritma K-Nearest Neighbors (KNN)"},{"location":"knn/#klasifikasi-terdekat-nearest-neighbor-classification","text":"Data baru yang diklasifikasi selanjutnya diproyeksikan pada ruang dimensi banyak yang telah memuat titik-titik c data pembelajaran. Proses klasifikasi dilakukan dengan mencari titik c terdekat dari c-baru ( nearest neighbor ) . Teknik pencarian tetangga terdekat yang umum dilakukan dengan menggunakan formula jarak euclidean . Berikut beberapa formula yang digunakan dalam algoritma knn.","title":"KLASIFIKASI TERDEKAT (NEAREST NEIGHBOR CLASSIFICATION)**"},{"location":"knn/#euclidean-distance","text":"Jarak Euclidean adalah formula untuk mencari jarak antara 2 titik dalam ruang dua dimensi.","title":"Euclidean Distance"},{"location":"knn/#hamming-distance","text":"Jarak Hamming adalah cara mencari jarak antar 2 titik yang dihitung dengan panjang vektor biner yang dibentuk oleh dua titik tersebut dalam block kode biner.","title":"Hamming Distance"},{"location":"knn/#manhattan-distance","text":"Manhattan Distance atau Taxicab Geometri adalah formula untuk mencari jarak d antar 2 vektor p,q pada ruang dimensi n .","title":"Manhattan Distance"},{"location":"knn/#minkowski-distance","text":"Minkowski distance adalah formula pengukuran antar 2 titik pada ruang vektor normal yang merupakan hibridisasi yang mengeneralisasi euclidean distance dan mahattan distance. Teknik pencarian tetangga terdekat disesuaikan dengan dimensi data, proyeksi, dan kemudahan implementasi oleh pengguna.","title":"Minkowski Distance"},{"location":"knn/#banyaknya-k-tetangga-terdekat","text":"Untuk menggunakan algoritma k nearest neighbors, perlu ditentukan banyaknya k tetangga terdekat yang digunakan untuk melakukan klasifikasi data baru. Banyaknya k, sebaiknya merupakan angka ganjil, misalnya k = 1, 2, 3, dan seterusnya. Penentuan nilai k dipertimbangkan berdasarkan banyaknya data yang ada dan ukuran dimensi yang dibentuk oleh data. Semakin banyak data yang ada, angka k yang dipilih sebaiknya semakin rendah. Namun, semakin besar ukuran dimensi data, angka k yang dipilih sebaiknya semakin tinggi.","title":"Banyaknya k Tetangga Terdekat"},{"location":"knn/#algoritma-k-nearest-neighbors","text":"Tentukan k bilangan bulat positif berdasarkan ketersediaan data pembelajaran. Pilih tetangga terdekat dari data baru sebanyak k. Tentukan klasifikasi paling umum pada langkah (ii), dengan menggunakan frekuensi terbanyak. Keluaran klasifikasi dari data sampel baru.","title":"Algoritma K-Nearest Neighbors"},{"location":"knn/#studi-kasus-data-pengunjung-perpustakaan-menggunakan-knn","text":"","title":"STUDI KASUS  Data pengunjung perpustakaan menggunakan knn"},{"location":"knn/#langkah-1-menyiapkan-tools-dan-bahan","text":"mempersiapkan sebuah data untuk dijadikan knn disini saya menggunakan data pengunjung perpustakaan yang berekstensi csv. tools yang digunakan : python 2.7","title":"Langkah 1 : menyiapkan tools dan bahan"},{"location":"knn/#langkah-2-import-libarary-yang-di-butuhkan","text":"import pandas as pd from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import train_test_split Memanggil file CSV data_orj = pd.read_csv( perpustakaan.csv ) Mengabil data untuk percobaan data = data_orj.loc [:,'Visitor':'book code'] Input jumlah k terdekat inK = int(input( Masukkan K : )) Mencari knn dengan menggunakan sklearn neighbors clssifier","title":"Langkah 2 : import libarary yang di butuhkan"},{"location":"knn/#langkah-3-mencari-knn-dengan-menggunakan-sklearn-neighbors-clssifier","text":"knn = KNeighborsClassifier(n_neighbors = inK) x,y = data_knn.loc[:,data_knn.columns != 'book code'], data_knn.loc[:,'book code'] x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state = 42) knn.fit(x_train,y_train) prediction = knn.predict(x_test) print('KNN dengan (K =',inK,') Akurasinya adalah: ', knn.score(x_test,y_test)) print( ) datatest = pd.DataFrame(x_test) datatest[ book code ] = y_test datatest[ prediksi ] = prediction print ( Data Asli dan Prediksinya ) print (datatest)","title":"Langkah 3 : Mencari knn dengan menggunakan sklearn neighbors clssifier"},{"location":"knn/#output-program","text":"","title":"Output program :"},{"location":"tree/","text":"Decision tree Konsep dari pohon keputusan adalah mengubah data menjadi decision tree dan aturan-aturan keputusan. Manfaat utama dari penggunaan decision tree adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simple, sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Nama lain dari decision tree adalah CART ( Classification and Regression Tree ). Dimana metode ini merupakan gabungan dari dua jenis pohon, yaitu classification tree dan juga regression tree . Untuk memudahkan, berikut ilustrasi dari keduanya. Kelebihan lain dari metode ini adalah mampu mengeliminasi perhitungan atau data-data yang kiranya tidak diperlukan. Sebab, sampel yang ada biasanya hanya diuji berdasarkan kriteria atau kelas tertentu saja. Meski memiliki banyak kelebihan, namun bukan berarti metode ini tidak memiliki kekurangan. Decision tree ini bisa terjadi overlap, terutama ketika kelas dan kriteria yang digunakan sangat banyak tentu saja dapat meningkatkan waktu pengambilan keputusan sesuai dengan jumlah memori yang dibutuhkan. Dalam hal akumulasi, decision tree juga seringkali mengalami kendala eror terutama dalam jumlah besar. Selain itu, terdapat pula kesulitan dalam mendesain decision tree yang optimal. Apalagi mengingat kualitas keputusan yang didapatkan dari metode decision tree sangat tergantung pada bagaimana pohon tersebut didesain. Terlepas dari kekurangan dan kelebihan dari decision tree , metode ini banyak digunakan lebih lanjut dalam berbagai pengolahan data. Mulai dari data mining dan juga machine learning . Dalam dunia kerja, decision tree sendiri sangat berguna untuk penilaian credit scoring. Jika anda pernah mengajukan kredit yang diproses secara instan, nah anda sudah mempunyai pengalaman dari decision tree . Contoh penerapan decision tree dalam perpustakan dengan menggunkan bahasa pemrograman python. Intsal tool yang dibutuhkan : Python 3.7 Jupyter Siapkan data yang akan dijadikan acuan, disini saya menggunkan data pengunjung perpustakaan, data harus berekstensi CSV. Import library yang dibutuhkan : import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn import metrics from sklearn.metrics import accuracy_score import seaborn as sns from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO from IPython.display import Image from sklearn.tree import export_graphviz import pydotplus import numpy as np Masukan file csv kedalam program data = pd.read_csv('perpustakaan.csv') data.head() data.info() Sesuaikan dengan kolom yang ada pada data csv zero_not_accepted = ['Visitor','Age','loan amount','book code'] # for col in zero_not_accepted: # for i in data[col]: # if i==0: # colSum = sum(data[col]) # meanCol=colSum/len(data[col]) # data[col]=meanCol for col in zero_not_accepted: data[col]= data[col].replace(0,np.NaN) mean = int(data[col].mean(skipna=True)) data[col] = data[col].replace(np.NaN,mean) x : menampilkankolom yang ingin di uji y : menampilkan class X = data.iloc[:,0:3] y = data.iloc[:,3] #Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age #build model train data X = data[['Visitor','Age','loan amount','book code']] y = data['Gendre'] #split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0) clf = DecisionTreeClassifier(criterion= entropy , max_depth=4) clf = clf.fit(X_train,y_train) y_pred = clf.predict(X_test) feature_cols = ['Visitor','Age','loan amount','book code'] dot_data = StringIO() menampilkan grafik export_graphviz(clf, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=['B','R','L']) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_png('a.png') Image(graph.create_png()) Hasil output :","title":"Decision tree"},{"location":"tree/#decision-tree","text":"Konsep dari pohon keputusan adalah mengubah data menjadi decision tree dan aturan-aturan keputusan. Manfaat utama dari penggunaan decision tree adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simple, sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Nama lain dari decision tree adalah CART ( Classification and Regression Tree ). Dimana metode ini merupakan gabungan dari dua jenis pohon, yaitu classification tree dan juga regression tree . Untuk memudahkan, berikut ilustrasi dari keduanya. Kelebihan lain dari metode ini adalah mampu mengeliminasi perhitungan atau data-data yang kiranya tidak diperlukan. Sebab, sampel yang ada biasanya hanya diuji berdasarkan kriteria atau kelas tertentu saja. Meski memiliki banyak kelebihan, namun bukan berarti metode ini tidak memiliki kekurangan. Decision tree ini bisa terjadi overlap, terutama ketika kelas dan kriteria yang digunakan sangat banyak tentu saja dapat meningkatkan waktu pengambilan keputusan sesuai dengan jumlah memori yang dibutuhkan. Dalam hal akumulasi, decision tree juga seringkali mengalami kendala eror terutama dalam jumlah besar. Selain itu, terdapat pula kesulitan dalam mendesain decision tree yang optimal. Apalagi mengingat kualitas keputusan yang didapatkan dari metode decision tree sangat tergantung pada bagaimana pohon tersebut didesain. Terlepas dari kekurangan dan kelebihan dari decision tree , metode ini banyak digunakan lebih lanjut dalam berbagai pengolahan data. Mulai dari data mining dan juga machine learning . Dalam dunia kerja, decision tree sendiri sangat berguna untuk penilaian credit scoring. Jika anda pernah mengajukan kredit yang diproses secara instan, nah anda sudah mempunyai pengalaman dari decision tree .","title":"Decision tree"},{"location":"tree/#contoh-penerapan-decision-tree-dalam-perpustakan-dengan-menggunkan-bahasa-pemrograman-python","text":"Intsal tool yang dibutuhkan : Python 3.7 Jupyter Siapkan data yang akan dijadikan acuan, disini saya menggunkan data pengunjung perpustakaan, data harus berekstensi CSV. Import library yang dibutuhkan : import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn import metrics from sklearn.metrics import accuracy_score import seaborn as sns from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO from IPython.display import Image from sklearn.tree import export_graphviz import pydotplus import numpy as np Masukan file csv kedalam program data = pd.read_csv('perpustakaan.csv') data.head() data.info() Sesuaikan dengan kolom yang ada pada data csv zero_not_accepted = ['Visitor','Age','loan amount','book code'] # for col in zero_not_accepted: # for i in data[col]: # if i==0: # colSum = sum(data[col]) # meanCol=colSum/len(data[col]) # data[col]=meanCol for col in zero_not_accepted: data[col]= data[col].replace(0,np.NaN) mean = int(data[col].mean(skipna=True)) data[col] = data[col].replace(np.NaN,mean) x : menampilkankolom yang ingin di uji y : menampilkan class X = data.iloc[:,0:3] y = data.iloc[:,3] #Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age #build model train data X = data[['Visitor','Age','loan amount','book code']] y = data['Gendre'] #split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0) clf = DecisionTreeClassifier(criterion= entropy , max_depth=4) clf = clf.fit(X_train,y_train) y_pred = clf.predict(X_test) feature_cols = ['Visitor','Age','loan amount','book code'] dot_data = StringIO() menampilkan grafik export_graphviz(clf, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=['B','R','L']) graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) graph.write_png('a.png') Image(graph.create_png()) Hasil output :","title":"Contoh penerapan decision tree dalam perpustakan dengan menggunkan bahasa pemrograman python."}]}